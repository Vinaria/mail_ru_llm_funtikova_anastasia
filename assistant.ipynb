{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle \n",
    "from itertools import chain\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "from typing import List, Dict, Optional, Iterable, Tuple\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет и займемся обработкой данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'RecipeNLG_dataset.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "ingr_df = df[['title', 'ingredients']].copy()\n",
    "direct_df = df[['title', 'directions']].copy()\n",
    "del df\n",
    "\n",
    "ingr_df = ingr_df.dropna()\n",
    "ingr_df['ingredients'] = ingr_df['ingredients'].apply(lambda x: '\\n-'.join([''] + json.loads(x)))\n",
    "ingr_df['text'] = ingr_df.title + ingr_df.ingredients\n",
    "del ingr_df['ingredients']\n",
    "del ingr_df['title']\n",
    "\n",
    "direct_df = direct_df.dropna()\n",
    "direct_df['directions'] = direct_df['directions'].apply(lambda x: ' '.join(json.loads(x)))\n",
    "direct_df['text'] = direct_df.title + ': ' + direct_df.directions\n",
    "del direct_df['directions']\n",
    "del direct_df['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StatLm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self,\n",
    "                 token_pattern: str = '\\w+|[\\!\\?\\,\\.\\-\\:\\/]',\n",
    "                 eos_token: str = '<EOS>',\n",
    "                 pad_token: str = '<PAD>',\n",
    "                 unk_token: str = '<UNK>'):\n",
    "        self.token_pattern = token_pattern\n",
    "        self.eos_token = eos_token\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        \n",
    "        self.special_tokens = [self.eos_token, self.pad_token, self.unk_token]\n",
    "        self.vocab = None\n",
    "        self.inverse_vocab = None\n",
    "    \n",
    "    def text_preprocess(self, input_text: str) -> str:\n",
    "        \"\"\" Предобрабатываем один текст \"\"\"\n",
    "        # input_text = ... # приведение к нижнему регистру\n",
    "        input_text = input_text.lower()\n",
    "        input_text = re.sub('\\s+', ' ', input_text) # унифицируем пробелы\n",
    "        input_text = input_text.strip()\n",
    "        return input_text\n",
    "    \n",
    "    def build_vocab(self, corpus: List[str]) -> None:\n",
    "        assert len(corpus)\n",
    "        all_tokens = set()\n",
    "        for text in corpus:\n",
    "            all_tokens |= set(self._tokenize(text, append_eos_token=False))\n",
    "        self.vocab = {elem: ind for ind, elem in enumerate(all_tokens)}\n",
    "        special_tokens = [self.eos_token, self.unk_token, self.pad_token]\n",
    "        for token in special_tokens:\n",
    "            self.vocab[token] = len(self.vocab)\n",
    "        self.inverse_vocab = {ind: elem for elem, ind in self.vocab.items()}\n",
    "        return self\n",
    "        \n",
    "    def _tokenize(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
    "        text = self.text_preprocess(text)\n",
    "        tokens = re.findall(self.token_pattern, text)\n",
    "        if append_eos_token:\n",
    "            tokens.append(self.eos_token)\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
    "        \"\"\" Токенизируем текст \"\"\"\n",
    "        tokens = self._tokenize(text, append_eos_token)\n",
    "        ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, input_ids: Iterable[int], remove_special_tokens: bool = False) -> str:\n",
    "        assert len(input_ids)\n",
    "        assert max(input_ids) < len(self.vocab) and min(input_ids) >= 0\n",
    "        tokens = []\n",
    "        for ind in input_ids:\n",
    "            token = self.inverse_vocab[ind]\n",
    "            if remove_special_tokens and token in self.special_tokens:\n",
    "                continue\n",
    "            tokens.append(token)\n",
    "        text = ' '.join( tokens )\n",
    "        return text\n",
    "    \n",
    "    def save(self, path: str) -> bool:\n",
    "        data = {\n",
    "            'token_pattern': self.token_pattern,\n",
    "            'eos_token': self.eos_token,\n",
    "            'pad_token': self.pad_token,\n",
    "            'unk_token': self.unk_token,\n",
    "            'special_tokens': self.special_tokens,\n",
    "            'vocab': self.vocab,\n",
    "            'inverse_vocab': self.inverse_vocab,\n",
    "        }\n",
    "        \n",
    "        with open(path, 'wb') as fout:\n",
    "            pickle.dump(data, fout)\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def load(self, path: str) -> bool:\n",
    "        with open(path, 'rb') as fin:\n",
    "            data = pickle.load(fin)\n",
    "            \n",
    "        self.token_pattern = data['token_pattern']\n",
    "        self.eos_token = data['eos_token']\n",
    "        self.pad_token = data['pad_token']\n",
    "        self.unk_token = data['unk_token']\n",
    "        self.special_tokens = data['special_tokens']\n",
    "        self.vocab = data['vocab']\n",
    "        self.inverse_vocab = data['inverse_vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationConfig:\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Тут можно задать любые параметры и их значения по умолчанию\n",
    "        Значения для стратегии декодирования decoding_strategy: ['max', 'top-p']\n",
    "        \"\"\"\n",
    "        self.temperature = kwargs.pop(\"temperature\", 1.0)\n",
    "        self.max_tokens = kwargs.pop(\"max_tokens\", 32)\n",
    "        self.sample_top_p = kwargs.pop(\"sample_top_p\", 0.9)\n",
    "        self.decoding_strategy = kwargs.pop(\"decoding_strategy\", 'max')\n",
    "        self.remove_special_tokens = kwargs.pop(\"remove_special_tokens\", False)\n",
    "        self.validate()\n",
    "        \n",
    "    def validate(self):\n",
    "        \"\"\" Здесь можно валидировать параметры \"\"\"\n",
    "        if not (1.0 > self.sample_top_p > 0):\n",
    "            raise ValueError('sample_top_p')\n",
    "        if self.decoding_strategy not in ['max', 'top-p']:\n",
    "            raise ValueError('decoding_strategy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatLM:\n",
    "    def __init__(self, \n",
    "                 tokenizer: Tokenizer,\n",
    "                 context_size: int = 2,\n",
    "                 alpha: float = 0.1\n",
    "                ):\n",
    "        \n",
    "        assert context_size >= 2\n",
    "        \n",
    "        self.context_size = context_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.n_gramms_stat = defaultdict(int)\n",
    "        self.nx_gramms_stat = defaultdict(int)\n",
    "        \n",
    "    def get_token_by_ind(ind: int) -> str:\n",
    "        return self.tokenizer.vocab.get(ind)\n",
    "    \n",
    "    def get_ind_by_token(token: str) -> int:\n",
    "        return self.tokenizer.inverse_vocab.get(token, self.tokenizer.inverse_vocab[self.unk_token])\n",
    "        \n",
    "    def train(self, train_texts: List[str]):\n",
    "        for sentence in tqdm(train_texts, desc='train lines'):\n",
    "            sentence_ind = self.tokenizer.encode(sentence)\n",
    "            for i in range(len(sentence_ind) - self.context_size):\n",
    "                \n",
    "                seq = tuple(sentence_ind[i: i + self.context_size - 1])\n",
    "                self.n_gramms_stat[seq] += 1\n",
    "                \n",
    "                seq_x = tuple(sentence_ind[i: i + self.context_size])\n",
    "                self.nx_gramms_stat[seq_x] += 1\n",
    "                \n",
    "            seq = tuple(sentence_ind[len(sentence_ind) - self.context_size:])\n",
    "            self.n_gramms_stat[seq] += 1\n",
    "            \n",
    "    def sample_token(self, \n",
    "                     token_distribution: np.ndarray,\n",
    "                     generation_config: GenerationConfig) -> int:\n",
    "        if generation_config.decoding_strategy == 'max':\n",
    "            return token_distribution.argmax()\n",
    "        elif generation_config.decoding_strategy == 'top-p':\n",
    "            token_distribution = sorted(list(zip(token_distribution, np.arange(len(token_distribution)))),\n",
    "                                        reverse=True)\n",
    "            total_proba = 0.0\n",
    "            tokens_to_sample = []\n",
    "            tokens_probas = []\n",
    "            for token_proba, ind in token_distribution:\n",
    "                tokens_to_sample.append(ind)\n",
    "                tokens_probas.append(token_proba)\n",
    "                total_proba += token_proba\n",
    "                if total_proba >= generation_config.sample_top_p:\n",
    "                    break\n",
    "            # для простоты отнормируем вероятности, чтобы суммировались в единицу\n",
    "            tokens_probas = np.array(tokens_probas) / generation_config.temperature\n",
    "            tokens_probas = tokens_probas / tokens_probas.sum()\n",
    "            return np.random.choice(tokens_to_sample, p=tokens_probas)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown decoding strategy: {generation_config.decoding_strategy}')\n",
    "            \n",
    "    def save_stat(self, path: str) -> bool:\n",
    "        stat = {\n",
    "            'n_gramms_stat': self.n_gramms_stat,\n",
    "            'nx_gramms_stat': self.nx_gramms_stat,\n",
    "            'context_size': self.context_size,\n",
    "            'alpha': self.alpha\n",
    "        }\n",
    "        with open(path, 'wb') as fout:\n",
    "            pickle.dump(stat, fout)\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def load_stat(self, path: str) -> bool:\n",
    "        with open(path, 'rb') as fin:\n",
    "            stat = pickle.load(fin)\n",
    "            \n",
    "        self.n_gramms_stat = stat['n_gramms_stat']\n",
    "        self.nx_gramms_stat = stat['nx_gramms_stat']\n",
    "        self.context_size = stat['context_size']\n",
    "        self.alpha = stat['alpha']\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def get_stat(self) -> Dict[str, Dict]:\n",
    "        \n",
    "        n_token_stat, nx_token_stat = {}, {}\n",
    "        for token_inds, count in self.n_gramms_stat.items():\n",
    "            n_token_stat[self.tokenizer.decode(token_inds)] = count\n",
    "        \n",
    "        for token_inds, count in self.nx_gramms_stat.items():\n",
    "            nx_token_stat[self.tokenizer.decode(token_inds)] = count\n",
    "        \n",
    "        return {\n",
    "            'n gramms stat': self.n_gramms_stat,\n",
    "            'n+1 gramms stat': self.nx_gramms_stat,\n",
    "            'n tokens stat': n_token_stat,\n",
    "            'n+1 tokens stat': nx_token_stat,\n",
    "        }\n",
    "    \n",
    "    def _get_next_token(self, \n",
    "                        tokens: List[int],\n",
    "                        generation_config: GenerationConfig) -> (int, str):\n",
    "        denominator = self.n_gramms_stat.get(tuple(tokens), 0) + self.alpha * len(self.tokenizer.vocab)\n",
    "        numerators = []\n",
    "        for ind in self.tokenizer.inverse_vocab:\n",
    "            numerators.append(self.nx_gramms_stat.get(tuple(tokens + [ind]), 0) + self.alpha)\n",
    "        \n",
    "        token_distribution = np.array(numerators) / denominator\n",
    "        max_proba_ind = self.sample_token(token_distribution, generation_config)\n",
    "        \n",
    "        next_token = self.tokenizer.inverse_vocab[max_proba_ind]\n",
    "        \n",
    "        return max_proba_ind, next_token\n",
    "            \n",
    "    def generate_token(self, \n",
    "                       text: str, \n",
    "                       generation_config: GenerationConfig\n",
    "                      ) -> Dict:\n",
    "        tokens = self.tokenizer.encode(text, append_eos_token=False)\n",
    "        tokens = tokens[-self.context_size + 1:]\n",
    "        \n",
    "        max_proba_ind, next_token = self._get_next_token(tokens, generation_config)\n",
    "        \n",
    "        return {\n",
    "            'next_token': next_token,\n",
    "            'next_token_num': max_proba_ind,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def generate_text(self, text: str, \n",
    "                      generation_config: GenerationConfig\n",
    "                     ) -> Dict:\n",
    "        \n",
    "        all_tokens = self.tokenizer.encode(text, append_eos_token=False)\n",
    "        tokens = all_tokens[-self.context_size + 1:]\n",
    "        \n",
    "        next_token = None\n",
    "        while next_token != self.tokenizer.eos_token and len(all_tokens) < generation_config.max_tokens:\n",
    "            max_proba_ind, next_token = self._get_next_token(tokens, generation_config)\n",
    "            all_tokens.append(max_proba_ind)\n",
    "            tokens = all_tokens[-self.context_size + 1:]\n",
    "        \n",
    "        new_text = self.tokenizer.decode(all_tokens, generation_config.remove_special_tokens)\n",
    "        \n",
    "        finish_reason = 'max tokens'\n",
    "        if all_tokens[-1] == self.tokenizer.vocab[self.tokenizer.eos_token]:\n",
    "            finish_reason = 'end of text'\n",
    "        \n",
    "        return {\n",
    "            'all_tokens': all_tokens,\n",
    "            'total_text': new_text,\n",
    "            'finish_reason': finish_reason\n",
    "        }\n",
    "    \n",
    "    def generate(self, text: str, generation_config: Dict) -> str:\n",
    "        return self.generate_text(text, generation_config)['total_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model():\n",
    "    config = {\n",
    "        'temperature': 1.0,\n",
    "        'max_tokens': 32,\n",
    "        'sample_top_p': 0.9,\n",
    "        'decoding_strategy': 'top-p',\n",
    "    }\n",
    "\n",
    "    stat_lm_path = 'models/stat_lm/stat_lm.pkl'\n",
    "    tokenizer_path = 'models/stat_lm/tokenizer.pkl'\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.load(tokenizer_path)\n",
    "        \n",
    "    stat_lm = StatLM(tokenizer)\n",
    "    stat_lm.load_stat(stat_lm_path)\n",
    "\n",
    "    generation_config = GenerationConfig(temperature=config['temperature'],\n",
    "                                         max_tokens=config['max_tokens'],\n",
    "                                         sample_top_p=config['sample_top_p'],\n",
    "                                         decoding_strategy=config['decoding_strategy'],\n",
    "                                         remove_special_tokens=True)\n",
    "\n",
    "    kwargs = {'generation_config': generation_config}\n",
    "    return stat_lm, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(temperature = 1.0, max_tokens = 32,\n",
    "                                     sample_top_p = 0.9, decoding_strategy = 'max',\n",
    "                                     remove_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Списки ингредиентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = np.array(ingr_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# building vocab\n",
    "\n",
    "tokenizer = Tokenizer().build_vocab(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "stat_lm = StatLM(tokenizer, context_size=2, alpha=0.1) # , sample_top_p = None\n",
    "stat_lm.train(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving data\n",
    "\n",
    "tokenizer.save('models/stat_lm/tokenizer.pkl')\n",
    "stat_lm.save_stat('models/stat_lm/stat_lm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инструкции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = np.array(direct_df.text)\n",
    "tokenizer = Tokenizer().build_vocab(train_texts)\n",
    "stat_lm = StatLM(tokenizer, context_size=2, alpha=0.1)\n",
    "stat_lm.train(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.save('models/stat_lm/tokenizer_2.pkl')\n",
    "stat_lm.save_stat('models/stat_lm/stat_lm_2.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
